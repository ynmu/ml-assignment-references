{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # your code here\n",
    "        matrix_w = []\n",
    "        \n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[ PART A ]** Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7f483253e7756e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sample Testing of implemented functions\n",
    "\n",
    "Use the below cells one by one to test each of your functions implemented above. <br>\n",
    "This should serve handy for debugging sections of your code <br>\n",
    "\n",
    "**Please Note - The hidden tests on which the actual points are awarded are different from these and usually run on a different, more complex dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69de0b2e5db3b495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c373a031a46afda",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07e377f58487a992",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_vec_act = pca_handler.compute_mean_vector(X_std_act)\n",
    "\n",
    "mean_vec_exp = [5.55111512, 2.77555756, 5.55111512, -5.55111512]\n",
    "\n",
    "mean_vec_act_tmp = mean_vec_act * 1e17\n",
    "\n",
    "assert pytest.approx(mean_vec_act_tmp, 0.1) == mean_vec_exp, \"Check compute_mean_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-58c635786e1213c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cov_mat_act = pca_handler.compute_cov(X_std_act, mean_vec_act) \n",
    "\n",
    "cov_mat_exp = [[ 1.33333333, 0.97573583, 0.44021511, 0.02776305],\n",
    " [ 0.97573583, 1.33333333, 0.88156376, 0.14760488],\n",
    " [ 0.44021511, 0.88156376, 1.33333333, -0.82029039],\n",
    " [ 0.02776305, 0.14760488, -0.82029039, 1.33333333]]\n",
    "\n",
    "assert pytest.approx(cov_mat_act, 0.01) == cov_mat_exp, \"Check compute_cov function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-62ec2c97eac4b335",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eig_vals_act, eig_vecs_act = pca_handler.compute_eigen_vector(cov_mat_act) \n",
    "\n",
    "eig_vals_exp = [2.96080083e+00, 1.80561744e+00, 5.66915059e-01, 7.86907276e-17]\n",
    "\n",
    "eig_vecs_exp = [[ 0.50989282,  0.38162981,  0.72815056,  0.25330765],\n",
    " [ 0.59707545,  0.33170546, -0.37363029, -0.62759286],\n",
    " [ 0.57599397, -0.37480162, -0.41446394,  0.59663585],\n",
    " [-0.22746684,  0.77708038, -0.3980161,   0.43126337]]\n",
    "\n",
    "assert pytest.approx(eig_vals_act, 0.01) == eig_vals_exp, \"Check compute_eigen_vector function\"\n",
    "\n",
    "for act, exp in zip(eig_vecs_act, eig_vecs_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check compute_eigen_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-14c3cd051410c833",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca_handler.feature_size = X.shape[1]\n",
    "var_exp_act = pca_handler.compute_explained_variance(eig_vals_act) \n",
    "\n",
    "var_exp_exp = [0.5551501556710813, 0.33855327084133857, 0.10629657348758019, 1.475451142706682e-17]\n",
    "\n",
    "assert pytest.approx(var_exp_act, 0.01) == var_exp_exp, \"Check compute_explained_variance function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b5a71e28a40a6cc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eig_pairs = np.array([(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "(1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "(0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])), \n",
    "(7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))])\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "\n",
    "matrix_w_exp = [[0.50989282, 0.38162981], \n",
    "                [0.59707545, 0.33170546], \n",
    "                [0.57599397, -0.37480162], \n",
    "                [-0.22746684, 0.77708038]]\n",
    "\n",
    "matrix_w_act = pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp)\n",
    "\n",
    "for act, exp in zip(matrix_w_act, matrix_w_exp):\n",
    "    assert pytest.approx(act, 0.001) == exp, \"Check compute_weight_matrix function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abb4fbdac246eac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Result Comparison with Sklearn\n",
    "\n",
    "The below cells should help you compare the output from your implementation against the sklearn implementation with a similar configuration. This is solely to help you validate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5ff5b5144a624a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to verify against the sklearn implementation given in the next cell\n",
    "pca_handler.transform_data(X_std=X_std_act, matrix_w=matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef8d927f60236e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn implementation to compare your results\n",
    "\n",
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA as pca1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling=StandardScaler()\n",
    " \n",
    "# Use fit and transform method\n",
    "# You may change the variable X if needed to verify against a different dataset\n",
    "print(\"Sample data:\", X)\n",
    "scaling.fit(X)\n",
    "Scaled_data=scaling.transform(X)\n",
    "print(\"\\nScaled data:\", Scaled_data)\n",
    " \n",
    "# Set the n_components=3\n",
    "principal=pca1(n_components=2)\n",
    "principal.fit(Scaled_data)\n",
    "x=principal.transform(Scaled_data)\n",
    " \n",
    "# Check the dimensions of data after PCA\n",
    "print(\"\\nTransformed Data\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
