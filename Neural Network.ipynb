{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29cc0557591f5119ed28df4b31530205",
     "grade": false,
     "grade_id": "cell-344b8948f17b670d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Grading\n",
    "The final score that you will receive for your programming assignment is generated in relation to the total points set in your programming assignment itemâ€”not the total point value in the nbgrader notebook.<br>\n",
    "When calculating the final score shown to learners, the programming assignment takes the percentage of earned points vs. the total points provided by nbgrader and returns a score matching the equivalent percentage of the point value for the programming assignment. <br>\n",
    "**DO NOT CHANGE VARIABLE OR METHOD SIGNATURES** The autograder will not work properly if you change the variable or method signatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f13a336c857430fd03a6ace8ff3a46",
     "grade": false,
     "grade_id": "cell-d3724f900290df95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Validate Button\n",
    "Please note that this assignment uses nbgrader to facilitate grading. You will see a **validate button** at the top of your Jupyter notebook. If you hit this button, it will run tests cases for the lab that aren't hidden. It is good to use the validate button before submitting the lab. Do know that the labs in the course contain hidden test cases. The validate button will not let you know whether these test cases pass. After submitting your lab, you can see more information about these hidden test cases in the Grader Output. <br>\n",
    "***Cells with longer execution times will cause the validate button to time out and freeze. Please know that if you run into Validate time-outs, it will not affect the final submission grading.*** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed7c693a3801ad7001a646a0b845090d",
     "grade": false,
     "grade_id": "cell-6f53374656359ce3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1. Neural Networks\n",
    "This assignment has mixed types of theoretical and code implementation questions on multilayer perceptron and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9807beb0f33d207bde175ddfcefc3555",
     "grade": false,
     "grade_id": "cell-e9c4e81b1f64c778",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Peer Review] Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "**Part A** : Answer this question in this week's Peer Review assignment. Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights and biases as applicable:\n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$\n",
    "\n",
    "ii. $~~x_1 \\texttt{ NOR } x_2$\n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fcdc9c7e4b0ff8c79728408fe31d206",
     "grade": false,
     "grade_id": "cell-ff045cbc37ce40ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B** : Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. <br>\n",
    "In this week's Peer Review, describe your architecture and state your weight matrices and bias vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e25dfa160b71663e07acb99a3f8ed35",
     "grade": false,
     "grade_id": "cell-18ccb1304f92244e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$. <br>\n",
    "Answer the questions about this section in this week's Peer Review assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75b8d9054ed2f46fa506ae7da83894f1",
     "grade": false,
     "grade_id": "cell-bfaabe5488f59154",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# implement forward propagation for network\n",
    "# show that it correctly produces the correct boolean output values \n",
    "# for each of the four possible combinations of x1 and x2 \n",
    "\n",
    "# Initialize x with the 4 possible combinations of 0 and 1 to generate 4 values for y(output)\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ccc87df79d764b8bd2ada03a2ffca6",
     "grade": false,
     "grade_id": "cell-ad9d1e323efa4b80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[15 points, Peer Review] Problem 2 - Back propagation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77f4b8a814fce7fb8a2f19f2b4ee7a87",
     "grade": false,
     "grade_id": "cell-c06610092b8715b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "580a73400e366b6514f08e8461c77d61",
     "grade": false,
     "grade_id": "cell-c8989b8ade2901fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. Answer the Peer Review question about this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7afad5a4c5c2be4464903b1fafcde38",
     "grade": false,
     "grade_id": "cell-ee509363d6c8add9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. Answer the Peer Review question about this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbefb76562e8afa6e64e885a0f9bc446",
     "grade": false,
     "grade_id": "cell-7f8e597e7e4b60ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART C** Implement following activation functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Relu: f($x$) = max(0, $x$)\n",
    "<br><br>\n",
    "\n",
    "* Sigmoid: f($x$) = $\\frac{1}{1 + e^{-x}}$\n",
    "<br><br>\n",
    "\n",
    "* Softmax: f($x_i$) = $\\frac{e^x_i}{\\sum_{j=1}^{n} e^{x_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8233dc81204ea9ceb6bf4d797c3c9d0",
     "grade": false,
     "grade_id": "cell-d0c234545c5fc035",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def relu(x):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "def soft_max(x):\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e5671f427a7dd7fad4788a0f83b7b67",
     "grade": true,
     "grade_id": "cell-abe86f4d2a055610",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Activation function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "# Test Relu function\n",
    "assert int(relu(-6.5)) == 0, \"Check relu function\"\n",
    "\n",
    "# Test Sigmoid function\n",
    "assert pytest.approx(sigmoid(0.3), 0.00001) == 0.574442516811659, \"Check sigmoid function\"\n",
    "\n",
    "# Test Softmax function\n",
    "assert pytest.approx(soft_max([5,7]), 0.00001) == [0.11920292, 0.88079708], \"Check softmax function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af687097dc603de58b0b89ca4b352dc0",
     "grade": true,
     "grade_id": "cell-0193f7345d99339c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests relu, sigmoid, and softmax functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "772c5485fdb83d943fa22f55025a910c",
     "grade": false,
     "grade_id": "cell-219d9e1514d5ceef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART D** Implement the following Loss functions:\n",
    "\n",
    "Formulas for activation functions\n",
    "\n",
    "* Mean squared error <br>\n",
    "Formula: MSE = (1/n) * Î£(yi - yÌ‚i)^2\n",
    "\n",
    "* Mean absolute error <br>\n",
    "Formula: MAE = (1/n) * Î£|yi - yÌ‚i|\n",
    "\n",
    "* Hinge Loss <br>\n",
    "Formula: L = max(0, 1 - yi * yÌ‚i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7946e3115d8ce0995d4d78d8febe9741",
     "grade": false,
     "grade_id": "cell-7837043e5e1e21a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat,y):\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "def mean_absolute_error(yhat,y):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "def hinge(yhat,y):\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd9ebe41307ec68147b74b72e1a91e80",
     "grade": true,
     "grade_id": "cell-d9363ea4e62e2041",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Error function tests\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "# and there are additional hidden test cases on which your notebook will be evaluated upon submission\n",
    "\n",
    "y_true = np.array([2, 3, -0.45])\n",
    "y_pred = np.array([1.5, 3, 0.2])\n",
    "\n",
    "# Test mean squared error function\n",
    "assert pytest.approx(mean_squared_error(y_pred,y_true), 0.00001) == 0.2241666666666667, \"Check mean_squared_error function\"\n",
    "\n",
    "# Test mean absolute error function\n",
    "assert pytest.approx(mean_absolute_error(y_pred,y_true), 0.00001) == 0.3833333333333333, \"Check mean_absolute_error function\"\n",
    "\n",
    "# Test hinge loss function\n",
    "assert pytest.approx(hinge(y_pred,y_true), 0.00001) == 0.36333333333333334, \"Check hinge loss function\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ee2747c222da2f29264feb1da78385d",
     "grade": true,
     "grade_id": "cell-503845fcfe0eb3d2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests mean_squared_error, mean_absolute_error, and hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bb4734dce6230601e9891138145b6a5",
     "grade": false,
     "grade_id": "cell-be3dee9da9a08ec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Peer Review] Problem 3 - Build a feed-forward neural network\n",
    "---\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement forward propagation, prediction, back propagation, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "\n",
    "The skeleton for the network class is below. Befor filling out the codes below, read the PART X instruction. The place you will complete the code is indicated as \"TODO\" in the code. Pleaes do not modify other parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8adc6c23d42fa75219491344883bcf5",
     "grade": false,
     "grade_id": "cell-4f59cb9a915b1ca3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros((n, 1)) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def grad_loss(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        # TODO: step 1. Initialize activation on initial layer to x \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        ## TODO: step 2-4. Loop over layers and compute activities and activations \n",
    "        ## Use Sigmoid activation function defined above\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # TODO: step 1. forward prop training example to fill in activities and activations \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # TODO: step 2. compute deltas on output layer (Hint: python index numbering starts from 0 ends at N-1)\n",
    "        # Correction in Instructions: From the instructions mentioned below for backward propagation,\n",
    "        # Use normal product instead of dot product in Step 2 and 6\n",
    "        # The derivative and gradient functions have already been implemented for you\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # TODO: step 3-6. loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        # your code here\n",
    "        \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None,\n",
    "              eta=0.25, num_epochs=10, isPrint=True, isVis=False):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs (step 1.)\n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples (step 2.) \n",
    "            for ind in shuffled_inds: \n",
    "                \n",
    "                # TODO: step 3. back prop to get derivatives \n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "                # TODO: step 4. update all weights and biases for all layers\n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "            # print mean loss every 10 epochs if requested \n",
    "            if isPrint and (ep % 10) == 0:\n",
    "                print(\"epoch {:3d}/{:3d}: \".format(ep, num_epochs), end=\"\")\n",
    "                print(\"  train loss: {:8.3f}\".format(self.compute_loss(X_train, y_train)), end=\"\")\n",
    "                if X_valid is not None:\n",
    "                    print(\"  validation loss: {:8.3f}\".format(self.compute_loss(X_valid, y_valid)))\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    \n",
    "            if isVis and (ep % 20) == 0:\n",
    "                self.pretty_pictures(X_train, y_train, decision_boundary=True, epoch=ep)\n",
    "                    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        compute average loss for given data set \n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        if len(y.shape) == 1:\n",
    "            y = y[np.newaxis, :]\n",
    "        for x, t in zip(X, y):\n",
    "            self.forward_prop(x)\n",
    "            if len(t.shape) == 1:\n",
    "                t = t.reshape(-1, 1)\n",
    "            loss += 0.5 * np.sum((self.a[-1] - t) ** 2)\n",
    "        return loss / X.shape[0]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h=1e-5):\n",
    "        \"\"\"\n",
    "        check whether the gradient is correct for X, y\n",
    "        \n",
    "        Assuming that back_prop has finished.\n",
    "        \"\"\"\n",
    "        for ll in range(self.L - 1):\n",
    "            oldW = self.W[ll].copy()\n",
    "            oldb = self.b[ll].copy()\n",
    "            for i in range(self.W[ll].shape[0]):\n",
    "                for j in range(self.W[ll].shape[1]):\n",
    "                    self.W[ll][i, j] = oldW[i, j] + h\n",
    "                    lxph = self.compute_loss(x, y)\n",
    "                    self.W[ll][i, j] = oldW[i, j] - h\n",
    "                    lxmh = self.compute_loss(x, y)\n",
    "                    grad = (lxph - lxmh) / (2 * h)\n",
    "                    assert abs(self.dW[ll][i, j] - grad) < 1e-5\n",
    "                    self.W[ll][i, j] = oldW[i, j]\n",
    "            for i in range(self.b[ll].shape[0]):\n",
    "                j = 0\n",
    "                self.b[ll][i, j] = oldb[i, j] + h\n",
    "                lxph = self.compute_loss(x, y)\n",
    "                self.b[ll][i, j] = oldb[i, j] - h\n",
    "                lxmh = self.compute_loss(x, y)\n",
    "                grad = (lxph - lxmh) / (2 * h)\n",
    "                assert abs(self.db[ll][i, j] - grad) < 1e-5\n",
    "                self.b[ll][i, j] = oldb[i, j]\n",
    "        \n",
    "            \n",
    "    def pretty_pictures(self, X, y, decision_boundary=False, epoch=None):\n",
    "        \"\"\"\n",
    "        Function to plot data and neural net decision boundary\n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        :param decision_boundary: whether or not to plot decision \n",
    "        :param epoch: epoch number for printing \n",
    "        \"\"\"\n",
    "        \n",
    "        mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\"}\n",
    "        colorlist = [c for (n,c) in mycolors.items()]\n",
    "        colors = [colorlist[np.argmax(yk)] for yk in y]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "        \n",
    "        if decision_boundary:\n",
    "            xx, yy = np.meshgrid(np.linspace(-1.25,1.25,300), np.linspace(-1.25,1.25,300))\n",
    "            grid = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "            grid_pred = np.zeros_like(grid[:,0])\n",
    "            for ii in range(len(grid_pred)):\n",
    "                self.forward_prop(grid[ii,:])\n",
    "                grid_pred[ii] = np.argmax(self.a[-1])\n",
    "            grid_pred = grid_pred.reshape(xx.shape)\n",
    "            cmap = ListedColormap([\n",
    "                colorConverter.to_rgba('steelblue', alpha=0.30),\n",
    "                colorConverter.to_rgba('#a76c63', alpha=0.30)])\n",
    "            plt.contourf(xx, yy, grid_pred, cmap=cmap)\n",
    "            if epoch is not None: plt.text(-1.23,1.15, \"epoch = {:d}\".format(epoch), fontsize=16)\n",
    "\n",
    "        plt.scatter(X[:,0], X[:,1], color=colors, s=100, alpha=0.9)\n",
    "        plt.axis('off')\n",
    "        \n",
    "def generate_data(N, config=\"checkerboard\"):\n",
    "    X = np.zeros((N,2))\n",
    "    y = np.zeros((N,2)).astype(int)\n",
    "    \n",
    "    if config==\"checkerboard\":\n",
    "        nps, sqlen = N//9, 2/3\n",
    "        ctr = 0\n",
    "        for ii in range(3):\n",
    "            for jj in range(3):\n",
    "                X[ctr * nps : (ctr + 1) * nps, :] = np.column_stack(\n",
    "                    (np.random.uniform(ii * sqlen +.05-1, (ii+1) * sqlen - .05 -1, size=nps),\n",
    "                     np.random.uniform(jj * sqlen +.05-1, (jj+1) * sqlen - .05 -1, size=nps))) \n",
    "                y[ctr*nps:(ctr+1)*nps,(3*ii+jj)%2] = 1 \n",
    "                ctr += 1\n",
    "                \n",
    "    if config==\"blobs\":            \n",
    "        X, yflat = datasets.make_blobs(n_samples=N, centers=[[-.5,.5],[.5,-.5]],\n",
    "                                       cluster_std=[.20,.20],n_features=2)\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk,:] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    \n",
    "    if config==\"circles\":\n",
    "        kk=0\n",
    "        while kk < N / 2:\n",
    "            sample = 2 * np.random.rand(2) - 1 \n",
    "            if np.linalg.norm(sample) <= .45:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([1,0])\n",
    "                kk += 1 \n",
    "        while kk < N:\n",
    "            sample = 2 * np.random.rand(2) - 1\n",
    "            dist = np.linalg.norm(sample)\n",
    "            if dist < 0.9 and dist > 0.55:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([0,1])\n",
    "                kk += 1\n",
    "                \n",
    "    if config==\"moons\":\n",
    "        X, yflat = datasets.make_moons(n_samples=N, noise=.05)\n",
    "        X[:,0] = .5 * (X[:,0] - .5)\n",
    "        X[:,1] = X[:,1] - .25\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk, :] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba8a9062eafc5921b6f48f3059061aeb",
     "grade": false,
     "grade_id": "cell-a1169ae25f8d2180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll be using our network to do binary classification of two-dimensional feature vectors.  Scroll down to the **Helper Functions** and examine the function ``generate_data``. Then mess around with the following cell to look at the various data sets available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f92ff3db3fa8f79a60d8d3eb0ee9ae2",
     "grade": false,
     "grade_id": "cell-8796d2ab35301fb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# play with this cell to look at the various data sets available\n",
    "# your code here\n",
    "\n",
    "nn = Network([2,3,2])\n",
    "X_train, y_train = generate_data(300, \"blobs\")\n",
    "nn.pretty_pictures(X_train, y_train, decision_boundary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee4fa946e983cf153374f13e101bc19c",
     "grade": false,
     "grade_id": "cell-ed7b994c9772a0dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " Go up to the ``__init__`` function in the ``Network Class``.  How are we initializing a network?  What data structures are we using to store things like weights, biases, deltas, etc? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "666fca44a1dc9fc1c5fab95c203e5ca8",
     "grade": false,
     "grade_id": "cell-fdb21fabf399efc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART A.** Implementing Forward Propagation. \n",
    "\n",
    "Complete the ``forward_prop`` function to implement forward propagation.  Your function should take in a single training example ``x`` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  Remember that the pseudocode that we wrote for forward-prop looked as follows: \n",
    "\n",
    "1. $\\quad$Initialize ${\\bf a}^0 = {\\bf x}$\n",
    "2. $\\quad$For $\\ell = 0, \\ldots, L-1$: \n",
    "3. $\\quad\\quad\\quad{\\bf z}^{\\ell+1} = W^\\ell {\\bf a}^\\ell + {\\bf b}^\\ell$\n",
    "3. $\\quad\\quad\\quad{\\bf a}^{\\ell+1} = g({\\bf z}^{\\ell+1})$\n",
    "\n",
    "When you think you're done, we can instantiate a ``Network`` with with $2$ neurons in the input layer, $3$ neurons in the sole hidden layer, and $2$ neurons in the output layer, and then forward prop one of the training examples. \n",
    "\n",
    "Check that your indexing was correct by making sure that all of the activations are now non-zero (remember, we initialized them to vectors of zeros). \n",
    "\n",
    "What other things could we check?<br>\n",
    "Answer the question about this section in this week's Peer Review assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "683d64e7490b34adc182caf546dd13cd",
     "grade": false,
     "grade_id": "cell-767905ab9692f8e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your forward_prop function\n",
    "nn = Network([2,3,2])\n",
    "nn.forward_prop(X_train[0])\n",
    "nn.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cabf84e7a646a18fd7f853eef92e75c6",
     "grade": false,
     "grade_id": "cell-ecb5d896a0389f91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART B.** Implementing Back Propagation\n",
    "\n",
    "OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases.  Remember, the pseudocode for back-prop was as follows: \n",
    "\n",
    "1. $\\quad$Forward propagate the training example ${\\bf x}$, ${\\bf y}$\n",
    "2. $\\quad$Compute the $\\delta^L = \\dfrac{\\partial \\mathscr{L}}{\\partial {\\bf a}^L} \\odot g'({\\bf z}^L)$\n",
    "3. $\\quad$For $\\ell = L-1, \\ldots, 1$: \n",
    "4. $\\quad\\quad\\quad \\dfrac{\\partial \\mathscr{L}}{\\partial W^\\ell} = \\delta^{\\ell+1} ({\\bf a}^\\ell)^T$\n",
    "5. $\\quad\\quad\\quad \\dfrac{\\partial \\mathscr{L}}{\\partial {\\bf b}^\\ell} = \\delta^{\\ell+1}$\n",
    "6. $\\quad\\quad\\quad\\delta^{\\ell} = (W^\\ell)^T\\delta^{\\ell+1} \\odot g'({\\bf z}^\\ell)$\n",
    "\n",
    "When you think you're done, instantiate a small ``Network`` and call back-prop for a single training example.  \n",
    "\n",
    "Check that it's likely working by checking that the derivative matrices ``dW`` and ``db`` are nonzero. <br>\n",
    "Answer the question about this section in this week's Peer Review assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08922f1b4829b160fb0036b65055f045",
     "grade": false,
     "grade_id": "cell-450534d4eefea854",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test back_prop\n",
    "nn = Network([2,3,2])\n",
    "nn.back_prop(X_train[0,:], y_train[0,:])\n",
    "print(nn.W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee835c3287997068112220b2c1940138",
     "grade": false,
     "grade_id": "cell-f9d911032fb2aa10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test gradient_check \n",
    "nn.gradient_check(X_train[0, :], y_train[0, :])\n",
    "print(nn.W[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6e786621257104cf6c282bd74880da9",
     "grade": false,
     "grade_id": "cell-0b14a3db8cca78bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The below test cells are to help you validate your forward and backward propagation functions better and help you identify problem areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cf6f7175086d717700c7b37d88f9d29",
     "grade": true,
     "grade_id": "cell-87b6b4d4b5259351",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network Tests - Forward Propagation\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "\n",
    "mock_X = np.array([[-0.4838731, 0.08083195], [0.93456167, -0.50316134]])\n",
    "np.random.seed(42)  ## DO NOT CHANGE THE SEED VALUE HERE\n",
    "nn1 = Network([2,3,2])\n",
    "nn1.forward_prop(mock_X)                 \n",
    "                 \n",
    "a = np.array([[0.],[0.]])\n",
    "b = np.array([[ 2.08587849, -0.31681043],[-0.94835809,  0.15999031],[-0.04793409,  0.92471859]])\n",
    "c = np.array([[ 0.24259536,  0.0874714 ],[-2.41978734, -1.98990137]])\n",
    "forward_z = [a, b, c]\n",
    "\n",
    "for pred, true in zip(nn1.z, forward_z):\n",
    "    assert pytest.approx(pred, 0.01) == true, \"Check forward function\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56706f8d80cb38659f089c8bb09d8133",
     "grade": true,
     "grade_id": "cell-ad565ceb6c1c4f33",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network Tests - Backward Propagation\n",
    "# PLEASE NOTE: These sample tests are only indicative and are added to help you debug your code\n",
    "\n",
    "mock_y = 0 * mock_X + 1\n",
    "np.random.seed(42)  ## DO NOT CHANGE THE SEED VALUE HERE\n",
    "nn1.back_prop(mock_X, mock_y)\n",
    "\n",
    "backward_w = [[-0.23413696, 1.57921282], [ 0.76743473, -0.46947439], [ 0.54256004, -0.46341769]]\n",
    "\n",
    "pred = nn1.W[0]\n",
    "true = backward_w\n",
    "\n",
    "assert pytest.approx(pred, 0.01) == true, \"Check backward function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b92441196065ee6db7f3fd50a46198b5",
     "grade": false,
     "grade_id": "cell-f88c764886880cb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note:** Next week, we will cover stochastic gradient descent. We encourage you to complete the following sections to train your model and get some results if you know how to do so. These sections are ungraded, so don't feel pressure to skip ahead a week in the material. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ce87a683078141d792388d816a283df",
     "grade": false,
     "grade_id": "cell-0466786367631c1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART C. [Ungraded]** Implementing trainning using stochastic gradient descent \n",
    "\n",
    "OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call back-prop to get the derivatives, and then update the weights and the biases via SGD. SGD uses minibatch to update weights. The training algorithm is following.\n",
    "1. For epoch = 0,1,...,N:\n",
    "2.     For (Xbatch, ybatch) in minibatches:\n",
    "3.         Compute gradients using backpropagation for the minibatch data\n",
    "4.         Update the weights (W, b) in the all layers (use loop over layer)    \n",
    "\n",
    "When you think you're done, execute the following code and watch the training loss evolve over the training process.  If you've done everything correctly, it'll hopefully go down! <br><br>\n",
    "Check out the solution in this week's Peer Review assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9062636f24110251b8f4bf7701b5907b",
     "grade": false,
     "grade_id": "cell-8d3e9ba04e6d283c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PART D.[Ungraded]**     \n",
    "OK! If you think you've worked out the bugs, let's start looking at the results. We'll build a simple neural network, train it on a training set, and watch the decision boundary of our classifier evolve to fit the data. We can do this by running similar code as above, but with the isVis flag set to True. Note that producing the plots takes considerable computational work, so things will go a bit slower now.\n",
    "\n",
    "Start with the blobs data set, and then move on to more complicated data sets like moons, circles, and finally the checkerboard. Note that for these more complicated geometries, it'll probably be necessary to chain the number of neurons in your hidden layer, or even add more hidden layers! <br>\n",
    "Check out the solution in this week's Peer Review assignment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
