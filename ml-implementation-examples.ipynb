{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These materials are provided as part of the course credit transfer request for CS 6140. For this file, the main purpose is to demonstrate the implementation of machine learning algorithms and theory-level concepts in our assignments.\n",
    "\n",
    "These are not all the assignments from the course, as some assessments were completed in virtual labs that are now inaccessible. However, I have tried to compile the available examples where we implemented algorithms by ourselves, rather than using external libraries directly.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-Nearest Neighbors Implementation\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - Non-parametric Models here](https://github.com/ynmu/ml-assignment-references/blob/main/Non-parametric%20Models.ipynb).\n",
    "\n",
    "\n",
    "#### Module\n",
    "\n",
    "Non-parametric Models, Supervised Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "In this assignment, we implemented our own KNN model from scratch instead of using the `KNeighborsClassifier` toolkit from `sklearn`. This was explicitly stated in the assignment requirement:\n",
    "\n",
    "> Do not use Scikit-Learn's KNeighborsClassifier in this problem. We're implementing this ourselves.\n",
    "\n",
    "The implementation includes the model constructor, along with the `classify`, `majority`, and `predict` methods. This process enhanced our understanding of KNN by reinforcing core concepts, especially in the `majority` method, where we count the frequency of each label and determine the most frequent label among the neighbors, which is central to KNN's functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, x_train, y_train, K=5):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param x_train: numpy array with shape (n_rows,1)- e.g. [[1,2],[3,4]]\n",
    "        :param y_train: numpy array with shape (n_rows,)- e.g. [1,-1]\n",
    "        :param K: The number of nearest points to consider in classification\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import and build the BallTree on training features \n",
    "        from sklearn.neighbors import BallTree\n",
    "        self.balltree = BallTree(x_train)\n",
    "        \n",
    "        # Cache training labels and parameter K \n",
    "        self.y_train = y_train\n",
    "        self.K = K \n",
    "        \n",
    "    def majority(self, neighbor_indices, neighbor_distances=None):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param neighbor_indices: The indices of the K nearest neighbors in self.X_train \n",
    "        :param neighbor_distances: Corresponding distances from query point to K nearest neighbors. \n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        if len(neighbor_indices) == 0:\n",
    "            return\n",
    "\n",
    "        yvals = self.y_train[neighbor_indices]\n",
    "        frequency = {}\n",
    "\n",
    "        if neighbor_distances is not None:\n",
    "            weights = 1 / neighbor_distances\n",
    "            for idx, val in enumerate(yvals[0]):\n",
    "                frequency[val] = frequency.get(val, 0) + weights[idx]\n",
    "        else:\n",
    "            for val in yvals[0]:\n",
    "                frequency[val] = frequency.get(val, 0) + 1\n",
    "\n",
    "        max_val = max(frequency.values())\n",
    "        max_labels = [key for key, val in frequency.items() if val == max_val]\n",
    "\n",
    "        if len(max_labels) > 1:\n",
    "            return max_labels[0]\n",
    "        else:\n",
    "            return max(frequency, key=frequency.get)\n",
    "\n",
    "    def classify(self, x):\n",
    "        \"\"\"\n",
    "        Given a query point, return the predicted label \n",
    "        \n",
    "        :param x: a query point stored as an ndarray  \n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here \n",
    "        dist, ind = self.balltree.query(x.reshape(1, -1), k = self.K)\n",
    "        \n",
    "        return self.majority(ind, dist)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an ndarray of query points, return yhat, an ndarray of predictions \n",
    "\n",
    "        :param X: an (m x p) dimension ndarray of points to predict labels for \n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        results = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            thispt = X[i, :].reshape(1, -1)\n",
    "            results.append(self.classify(thispt))\n",
    "        \n",
    "        A = np.ndarray([])\n",
    "        return np.append(results, A)[:-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. AdaBoost Classifier Implementation\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - Ensemble here](https://github.com/ynmu/ml-assignment-references/blob/main/Ensemble.ipynb).\n",
    "\n",
    "#### Module\n",
    "\n",
    "Ensemble methods, Supervised Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "In this assignment, we didn't use libraries like `sklearn.ensemble.AdaBoostClassifier`. Instead, we implemented our own AdaBoost classifier.\n",
    "\n",
    "The implementation includes the model constructor, along with the `fit`, `error_rate`, `predict`, `score`, and `staged_score` methods. The requirement reviewed the process of the AdaBoost algorithm and key formulas thoroughly. For example, it was stated in the assignment:\n",
    "\n",
    "> Recall that the model we attempt to learn in AdaBoost is given by \n",
    ">\n",
    "> $$\n",
    "> H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "> $$\n",
    "\n",
    "This process helped us better understand ensemble learning by showing how to iteratively train weak learners, adjust their importance based on performance, and focus on the hardest-to-classify instances, ultimately combining weak learners to build a stronger classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=3), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        w = np.ones(len(y_train), dtype=np.float128) / len(y_train)\n",
    "\n",
    "        for k in range(self.n_learners):\n",
    "            h = deepcopy(self.base)\n",
    "            h.fit(X_train, y_train, sample_weight=w)\n",
    "            \n",
    "            y_pred = h.predict(X_train)\n",
    "            errk = self.error_rate(y_train, y_pred, w)\n",
    "\n",
    "            if errk == 0:\n",
    "                alpha_k = np.inf\n",
    "            elif errk == 1:\n",
    "                alpha_k = -np.inf\n",
    "            else:\n",
    "                alpha_k = 0.5 * np.log((1 - errk) / errk)\n",
    "\n",
    "            w *= np.exp(-alpha_k * y_train * y_pred)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # store the alpha and the weak learner\n",
    "            self.alpha[k] = alpha_k\n",
    "            self.learners.append(h)\n",
    "        \n",
    "        return self  \n",
    "            \n",
    "    def error_rate(self, y_true, y_pred, weights):\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Implement the weighted error rate\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        \n",
    "        numerator = copy.deepcopy(weights)\n",
    "        for i in range(len(numerator)):\n",
    "            numerator[i] = weights[i] * (y_true[i] != y_pred[i])\n",
    "        \n",
    "        error = numerator.sum() / weights.sum()\n",
    "        \n",
    "        return error\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        yhat = np.zeros(X.shape[0])\n",
    "\n",
    "        if len(self.learners) == 0:\n",
    "            raise Exception(\"Training not started\")\n",
    "\n",
    "        for learner, alpha in zip(self.learners, self.alpha):\n",
    "            yhat += alpha * learner.predict(X)\n",
    "\n",
    "        yhat = np.sign(yhat)\n",
    "        return yhat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        y_pred = self.predict(X)\n",
    "        correct = np.sum(y_pred == y)\n",
    "        return correct / len(y)\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        scores = []\n",
    "        \n",
    "        if len(self.learners) == 0:\n",
    "            raise Exception(\"AdaBoost Classifier has not yet been trained! Please train first.\")\n",
    "\n",
    "        for i in range(len(self.learners)):\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "\n",
    "            for j in range(i+1):\n",
    "                yhat += self.alpha[j] * self.learners[j].predict(X)\n",
    "\n",
    "            yhat = np.sign(yhat)\n",
    "\n",
    "            correct = np.sum(yhat == y)\n",
    "            scores.append(correct / len(y))\n",
    "\n",
    "        return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Random Forest Classifier Implementation\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - Ensemble here](https://github.com/ynmu/ml-assignment-references/blob/main/Ensemble.ipynb).\n",
    "\n",
    "#### Module\n",
    "\n",
    "Ensemble methods, Supervised Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "In this assignment, we implemented our own Random Forest classifier, without using libraries like `sklearn.ensemble.RandomForestClassifier`. The algorithm involves training an ensemble of decision trees and aggregating their predictions through majority voting.\n",
    "\n",
    "The implementation includes the model constructor, along with the `create_tree`, `predict`, and `score` methods. The requirement clearly outlined the process of training a Random Forest model and emphasized the following steps:\n",
    "\n",
    "> Remember that training the random forest algorithms involves the following steps: \n",
    "> \n",
    "> `for k = 1 to K:`\n",
    "> \n",
    "> $~~~~~~~~$ `a) Build the kth tree of depth d`\n",
    "> \n",
    "> $~~~~~~~~$ `b) Train the kth tree on a subset of the dataset with random feature splits`\n",
    ">\n",
    "> Predicting the classification result on new data involves returning the majority vote by all the trees in the random forest.\n",
    "\n",
    "#### Accompanying Written Exercises\n",
    "\n",
    "Build a random forest classifier and train it on the MNIST data to classify 3s and 8s in the cell below. Then see how the classifier performs on the test data by computing the misclassification error. (Remember: error = 1-score)\n",
    "\n",
    "What was the misclassification error for your random forest classifier? How did the misclassification error for the random forest classifier compare to the adaBoost classifier? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, x, y, sample_sz, n_trees=200, n_features='sqrt', max_depth=10, min_samples_leaf=5):\n",
    "        \"\"\"\n",
    "        Create a new random forest classifier.\n",
    "        \n",
    "        Args:\n",
    "            x : Input Feature vector\n",
    "            y : Corresponding Labels\n",
    "            sample_sz : Sample size\n",
    "            n_trees : Number of trees to ensemble\n",
    "            n_features : Method to select subset of features \n",
    "            max_depth : Maximum depth of the trees in the ensemble\n",
    "            min_sample_leaf : Minimum number of samples per leaf \n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "\n",
    "        self.features_set = []\n",
    "        self.x, self.y, self.sample_sz, self.max_depth, self.min_samples_leaf  = x, y, sample_sz, max_depth, min_samples_leaf\n",
    "        self.trees = [self.create_tree(i) for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self,i):\n",
    "        \"\"\"\n",
    "        create a single decision tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        idxs = np.asarray(idxs)\n",
    "\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        f_idxs = np.asarray(f_idxs)\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            self.features_set = np.array(f_idxs, ndmin=2)\n",
    "        else:\n",
    "            self.features_set = np.append(self.features_set, np.array(f_idxs,ndmin=2),axis=0)\n",
    "        \n",
    "        # =================================================================\n",
    "        # TODO: build a decision tree classifier and train it with x and y that is a subset of data (use idxs and f_idxs)\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth = self.max_depth, min_samples_leaf = self.min_samples_leaf)\n",
    "        x, y = self.x[idxs, :][:, f_idxs], self.y[idxs]\n",
    "        clf.fit(x, y)\n",
    "        \n",
    "        return clf\n",
    "       \n",
    "    def predict(self, x):\n",
    "        \n",
    "        # =================================================================\n",
    "        # TODO: create a vector of predictions  and return\n",
    "        # You will have to return the predictions of the final ensembles based on the individual trees' predicitons\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "\n",
    "        res = np.zeros(x.shape[0])\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            predictions = {}\n",
    "            for j in range(len(self.trees)):\n",
    "                tree = self.trees[j]\n",
    "                these_features = self.features_set[j]\n",
    "                this_prediction = tree.predict(x[i, these_features].reshape(1, -1))[0]\n",
    "                if this_prediction not in predictions.keys():\n",
    "                    predictions.update({this_prediction : 1})\n",
    "                else:\n",
    "                    previous_count = predictions[this_prediction]\n",
    "                    predictions.update({this_prediction : previous_count + 1})\n",
    "                    \n",
    "            current_best = (None, 0)\n",
    "            for key, value in predictions.items():\n",
    "                if value > current_best[1]:\n",
    "                    current_best = (key, value)\n",
    "                    \n",
    "            res[i] = current_best[0]\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # =================================================================\n",
    "        # TODO: Compute the score using the predict function and true labels y\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "\n",
    "        \n",
    "        yhat = self.predict(X)\n",
    "        correct_predictions = np.sum(yhat == y)\n",
    "        total_predictions = len(y)\n",
    "        acc = correct_predictions / total_predictions\n",
    "\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. PCA Implementation\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - PCA here](https://github.com/ynmu/ml-assignment-references/blob/main/PCA.ipynb).\n",
    "\n",
    "#### Module\n",
    "\n",
    "PCA, Unsupervised Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "In this assignment, we implemented our own PCA (Principal Component Analysis) algorithm instead of using existing libraries like `sklearn.decomposition.PCA`. This helped in gaining a deeper understanding of dimensionality reduction techniques and the mathematics behind PCA.\n",
    "\n",
    "Before moving onto the implementation, the key points of the PCA algorithm were reviewed in the assignment requirement:\n",
    "\n",
    ">\n",
    "> The gist of PCA Algorithm to compute principal components is follows:\n",
    "> - Calculate the covariance matrix X of data points.\n",
    "> - Calculate eigenvectors and corresponding eigenvalues.\n",
    "> - Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "> - Choose first k eigenvectors which satisfies target explained variance.\n",
    "> - Transform the original data of shape m observations times n features into m observations times k selected features.\n",
    ">\n",
    "\n",
    "We also manually calculated the `mean` and `covariance` matrix, with the formulas we learnt from class:\n",
    "$$\n",
    "\\mu_i = \\frac{1}{m} \\sum_{k=1}^{m} X_{k,i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_i, X_j) = \\frac{1}{m-1} \\sum_{k=1}^{m} (X_{k,i} - \\mu_i)(X_{k,j} - \\mu_j)\n",
    "$$\n",
    "\n",
    "\n",
    "This process helped us understand how PCA reduces the dimensionality of the dataset by projecting it onto the directions of maximum variance, while retaining the most important features based on the explained variance criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "\n",
    "        return StandardScaler().fit_transform(X)\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        m = X_std.shape[0]\n",
    "        mean_vector = X_std.sum(axis=0) / m\n",
    "        return mean_vector\n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        m, n = X_std.shape\n",
    "        cov_matrix = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                cov_ij = 0\n",
    "                for k in range(m):\n",
    "                    cov_ij += (X_std[k, i] - mean_vec[i]) * (X_std[k, j] - mean_vec[j])\n",
    "                cov_matrix[i, j] = cov_ij / (m - 1) # normalize by m-1\n",
    "        \n",
    "        return cov_matrix\n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        return np.linalg.eig(cov_mat)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        total = sum(eigen_vals)\n",
    "        var_exp = [(i / total) for i in sorted(eigen_vals, reverse=True)]\n",
    "        return var_exp\n",
    "\n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        matrix_w = []\n",
    "        for i, (_, eig_vec) in enumerate(eig_pairs):\n",
    "            if cum_var_exp[i] >= self.target_explained_variance:\n",
    "                break\n",
    "            matrix_w.append(eig_vec.reshape(-1, 1))\n",
    "        return np.hstack(matrix_w)\n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        \n",
    "        X_std = self.standardize(X)\n",
    "        mean_vec = self.compute_mean_vector(X_std)\n",
    "        cov_mat = self.compute_cov(X_std, mean_vec)\n",
    "        \n",
    "        eigen_vals, eigen_vecs = self.compute_eigen_vector(cov_mat)\n",
    "        var_exp = self.compute_explained_variance(eigen_vals)\n",
    "        cum_var_exp = self.cumulative_sum(var_exp)\n",
    "        \n",
    "        eig_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
    "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs, cum_var_exp)\n",
    "        \n",
    "        print(f\"weight matrix dimensions: {len(matrix_w)} x {len(matrix_w[0])}\")\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing our own PCA algorithm, we were also required to compare our implementation with the external library to validate the preformance. Below is quoted from the assignment instructions:\n",
    "\n",
    "> **Result Comparison with Sklearn**\n",
    ">\n",
    "> The below cells should help you compare the output from your implementation against the sklearn implementation with a similar configuration. This is solely to help you validate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn implementation to compare your results\n",
    "\n",
    "from sklearn.decomposition import PCA as ExternalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling = StandardScaler()\n",
    " \n",
    "# Use fit and transform method\n",
    "# You may change the variable X if needed to verify against a different dataset\n",
    "print(\"Sample data:\", X)\n",
    "scaling.fit(X)\n",
    "Scaled_data=scaling.transform(X)\n",
    "print(\"\\nScaled data:\", Scaled_data)\n",
    " \n",
    "# Set the n_components=3\n",
    "principal=ExternalPCA(n_components=2)\n",
    "principal.fit(Scaled_data)\n",
    "x=principal.transform(Scaled_data)\n",
    " \n",
    "# Check the dimensions of data after PCA\n",
    "print(\"\\nTransformed Data\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Recommender Systems\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - Recommender System here](https://github.com/ynmu/ml-assignment-references/blob/main/Recommender%20Systems.ipynb).\n",
    "\n",
    "\n",
    "#### Module\n",
    "\n",
    "Recommender Systems, Unsupervised Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "In this assignment, we implemented various recommender system algorithms without relying on pre-built libraries. In summary, we built the baseline `class RecSys`, and built `class ContentBased` and `class Collaborative` which inherit `class RecSys` and further add methods calculating item-item similarity matrix.\n",
    "\n",
    "Due to the extensive amount of code in this module, I am not including the full code here. Notably, we also reviewed some fundamental concepts like Cosine Similarity and Jaccard Similarity, with the formulas given:\n",
    "\n",
    "$$Cossim(A, B) = \\frac{(A · B)}{(||A|| · ||B||)}$$\n",
    "and\n",
    "$$Jacsim(A, B) = \\frac{|A ∩ B|}{|A ∪ B|}$$\n",
    "\n",
    "#### Accompanying Written Exercises\n",
    "\n",
    "Discuss which method(s) work better than others and why.\n",
    "\n",
    "Write your response in the textbox below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys():\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        self.allusers = list(self.data.users['uID'])\n",
    "        self.allmovies = list(self.data.movies['mID'])\n",
    "        self.genres = list(self.data.movies.columns.drop(['mID', 'title', 'year']))\n",
    "        self.mid2idx = dict(zip(self.data.movies.mID,list(range(len(self.data.movies)))))\n",
    "        self.uid2idx = dict(zip(self.data.users.uID,list(range(len(self.data.users)))))\n",
    "        self.Mr=self.rating_matrix()\n",
    "        self.Mm=None \n",
    "        self.sim=np.zeros((len(self.allmovies),len(self.allmovies)))\n",
    "        \n",
    "    def rating_matrix(self):\n",
    "        \"\"\"\n",
    "        Convert the rating matrix to numpy array of shape (#allusers,#allmovies)\n",
    "        \"\"\"\n",
    "        ind_movie = [self.mid2idx[x] for x in self.data.train.mID] \n",
    "        ind_user = [self.uid2idx[x] for x in self.data.train.uID]\n",
    "        rating_train = list(self.data.train.rating)\n",
    "        \n",
    "        return np.array(coo_matrix((rating_train, (ind_user, ind_movie)), shape=(len(self.allusers), len(self.allmovies))).toarray())\n",
    "\n",
    "\n",
    "    def predict_everything_to_3(self):\n",
    "        \"\"\"\n",
    "        Predict everything to 3 for the test data\n",
    "        \"\"\"\n",
    "        # Generate an array with 3s against all entries in test dataset\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        return np.full(len(self.data.test), 3)\n",
    "        \n",
    "        \n",
    "    def predict_to_user_average(self):\n",
    "        \"\"\"\n",
    "        Predict to average rating for the user.\n",
    "        Returns numpy array of shape (#users,)\n",
    "        \"\"\"\n",
    "        # Generate an array as follows:\n",
    "        # 1. Calculate all avg user rating as sum of ratings of user across all movies/number of movies whose rating > 0\n",
    "        # 2. Return the average rating of users in test data\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        user_avg = self.Mr.sum(axis=1) / (self.Mr > 0).sum(axis=1)\n",
    "\n",
    "        test_users = self.data.test['uID'].map(self.uid2idx)\n",
    "        return user_avg[test_users]\n",
    "    \n",
    "    \n",
    "    def predict_from_sim(self,uid,mid):\n",
    "        \"\"\"\n",
    "        Predict a user rating on a movie given userID and movieID\n",
    "        \"\"\"\n",
    "        # Predict user rating as follows:\n",
    "        # 1. Get entry of user id in rating matrix\n",
    "        # 2. Get entry of movie id in sim matrix\n",
    "        # 3. Employ 1 and 2 to predict user rating of the movie\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        user_idx = self.uid2idx[uid]\n",
    "        movie_idx = self.mid2idx[mid]\n",
    "        \n",
    "        uf = self.Mr[user_idx]\n",
    "        mf = self.sim[movie_idx]\n",
    "        \n",
    "        return np.dot(uf, mf) / np.dot(mf, uf > 0)\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict ratings in the test data. Returns predicted rating in a numpy array of size (# of rows in testdata,)\n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in self.data.test.iterrows():\n",
    "            uid, mid = row['uID'], row['mID']\n",
    "            predictions.append(self.predict_from_sim(uid, mid))\n",
    "\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    \n",
    "    def rmse(self,yp):\n",
    "        yp[np.isnan(yp)] = 3 #In case there is nan values in prediction, it will impute to 3.\n",
    "        yt = np.array(self.data.test.rating)\n",
    "        return np.sqrt(((yt-yp)**2).mean())\n",
    "\n",
    "    \n",
    "class ContentBased(RecSys):\n",
    "    def __init__(self,data):\n",
    "        super().__init__(data)\n",
    "        self.data=data\n",
    "        self.Mm = self.calc_movie_feature_matrix()  \n",
    "        \n",
    "    def calc_movie_feature_matrix(self):\n",
    "        \"\"\"\n",
    "        Create movie feature matrix in a numpy array of shape (#allmovies, #genres) \n",
    "        \"\"\"\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        return self.data.movies[self.genres].values\n",
    "    \n",
    "    def calc_item_item_similarity(self):\n",
    "        \"\"\"\n",
    "        Create item-item similarity using Jaccard similarity\n",
    "        \"\"\"\n",
    "        # Update the sim matrix by calculating item-item similarity using Jaccard similarity\n",
    "        # Jaccard Similarity: J(A, B) = |A∩B| / |A∪B| \n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        n = len(self.allmovies) \n",
    "        X = self.Mm.T\n",
    "        csr0 = csr_matrix((X > 0).astype(int))\n",
    "        intersec = np.array(csr0.T.dot(csr0).toarray()).astype(int)   \n",
    "        rowsum = X.sum(axis=0)\n",
    "        rsumtile = np.repeat(rowsum[:, np.newaxis], n, axis=1)\n",
    "        union = rsumtile.T + rsumtile - intersec\n",
    "        # avoid division by zero\n",
    "        self.sim = np.divide(intersec, union, where=(union != 0))\n",
    "        # self-similarity\n",
    "        np.fill_diagonal(self.sim, 1)\n",
    "        \n",
    "                \n",
    "class Collaborative(RecSys):    \n",
    "    def __init__(self,data):\n",
    "        super().__init__(data)\n",
    "        \n",
    "    def calc_item_item_similarity(self, simfunction, *X):  \n",
    "        \"\"\"\n",
    "        Create item-item similarity using similarity function. \n",
    "        X is an optional transformed matrix of Mr\n",
    "        \"\"\"    \n",
    "        # General function that calculates item-item similarity based on the sim function and data inputed\n",
    "        if len(X)==0:\n",
    "            self.sim = simfunction()            \n",
    "        else:\n",
    "            self.sim = simfunction(X[0]) # *X passes in a tuple format of (X,), to X[0] will be the actual transformed matrix\n",
    "\n",
    "    def cossim(self):    \n",
    "        \"\"\"\n",
    "        Calculates item-item similarity for all pairs of items using cosine similarity (values from 0 to 1) on utility matrix\n",
    "        Returns a cosine similarity matrix of size (#all movies, #all movies)\n",
    "        \"\"\"\n",
    "        # Return a sim matrix by calculating item-item similarity for all pairs of items using Jaccard similarity\n",
    "        # Cosine Similarity: C(A, B) = (A.B) / (||A||.||B||) \n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        mu = self.Mr.sum(axis=1) / (self.Mr>0).sum(axis=1)\n",
    "        mu_array = np.repeat(np.expand_dims(mu, axis=1), self.Mr.shape[1], axis=1)\n",
    "        X = self.Mr+(self.Mr==0) * mu_array-mu_array \n",
    "        \n",
    "        norm_X = np.linalg.norm(X, axis=0, keepdims=True)\n",
    "        Y = np.divide(X, norm_X, where=(norm_X != 0))\n",
    "\n",
    "        cos_sim = np.dot(Y.T, Y)\n",
    "        # fill the diagonal with 1\n",
    "        for i in range(len(self.allmovies)):\n",
    "            cos_sim[i, i]=1\n",
    "\n",
    "        print('Total time:', time.perf_counter() - t0)\n",
    "        return 0.5 + 0.5 * cos_sim\n",
    "    \n",
    "    def jacsim(self,Xr):\n",
    "        \"\"\"\n",
    "        Calculates item-item similarity for all pairs of items using jaccard similarity (values from 0 to 1)\n",
    "        Xr is the transformed rating matrix.\n",
    "        \"\"\"    \n",
    "        # Return a sim matrix by calculating item-item similarity for all pairs of items using Jaccard similarity\n",
    "        # Jaccard Similarity: J(A, B) = |A∩B| / |A∪B| \n",
    "\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        n = Xr.shape[1]\n",
    "        max_rating = int(Xr.max())\n",
    "\n",
    "        if max_rating > 1:\n",
    "            intersec = np.zeros((n, n)).astype(int)\n",
    "            for i in range(1, max_rating + 1):\n",
    "                csr = csr_matrix((Xr == i).astype(int))\n",
    "                intersec = intersec + np.array(csr.T.dot(csr).toarray()).astype(int)\n",
    "\n",
    "        csr_binary = csr_matrix((Xr > 0).astype(int))\n",
    "        intersec_binary = np.array(csr_binary.T.dot(csr_binary).toarray()).astype(int)\n",
    "\n",
    "        A = (Xr > 0).astype(bool)\n",
    "        rowsum = A.sum(axis=0)\n",
    "        rsumtile = np.repeat(rowsum.reshape((n, 1)), n, axis=1)\n",
    "        union = rsumtile.T + rsumtile - intersec_binary\n",
    "\n",
    "        if max_rating > 1:\n",
    "            jac_sim = intersec / union\n",
    "        else:\n",
    "            jac_sim = intersec_binary / union\n",
    "\n",
    "        if np.isnan(jac_sim).sum() > 0:\n",
    "            jac_sim[np.isnan(jac_sim)] = 0\n",
    "            for i in range(n):\n",
    "                jac_sim[i, i] = 1\n",
    "\n",
    "        return jac_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Feedforward Neural Network\n",
    "\n",
    "#### Original Instructions\n",
    "\n",
    "Please access the [full assignment - Neural Network here](https://github.com/ynmu/ml-assignment-references/blob/main/Neural%20Network.ipynb).\n",
    "\n",
    "#### Module\n",
    "\n",
    "Neural Networks, Deep Learning\n",
    "\n",
    "#### Description\n",
    "\n",
    "\n",
    "In this assignment, we implemented a general feed-forward neural network from scratch, without relying on deep learning libraries like TensorFlow or PyTorch. The network utilized sigmoid activation functions and included key methods for `forward propagation`, `back propagation`, and `train` using stochastic gradient descent (SGD). \n",
    "\n",
    "The assignment provided the signatures of the neural network class, which we were tasked with completing. We specifically focused on the implementation of forward propagation through layers of the network, calculating gradients during backpropagation, and updating the weights and biases using SGD.\n",
    "\n",
    "The assignment instructions stated:\n",
    "\n",
    "> \n",
    "> In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "> \n",
    "> The skeleton for the network class is below. Befor filling out the codes below, read the PART X instruction. The place you will complete the code is indicated as \"TODO\" in the code. Pleaes do not modify other parts of the code.\n",
    "\n",
    "\n",
    "The completed model also included helper functions such as gradient checking (`gradient_check`) to ensure that backpropagation was implemented correctly, and visualization (`pretty_pictures`) to plot decision boundaries and show the network's progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # bias vectors and matrices of weights\n",
    "        self.b = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        self.W = [np.random.randn(n, m) for (m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # derivatives of biases and weights for backprop\n",
    "        self.db = [np.zeros((n, 1)) for n in self.sizes[1:]]\n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # activities and activations\n",
    "        self.z = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        self.a = [np.zeros((n, 1)) for n in self.sizes]\n",
    "\n",
    "        # deltas\n",
    "        self.delta = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def grad_loss(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        # =================================================================\n",
    "        # TODO: step 1. Initialize activation on initial layer to x \n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        self.a[0] = x\n",
    "        \n",
    "        \n",
    "        # =================================================================\n",
    "        # TODO: step 2-4. Loop over layers and compute activities and activations \n",
    "        # Use Sigmoid activation function defined above\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "        \n",
    "        \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO: step 1. forward prop training example to fill in activities and activations \n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO: step 2. compute deltas on output layer (Hint: python index numbering starts from 0 ends at N-1)\n",
    "        # Correction in Instructions: From the instructions mentioned below for backward propagation,\n",
    "        # Use normal product instead of dot product in Step 2 and 6\n",
    "        # The derivative and gradient functions have already been implemented for you\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        self.delta[self.L - 1] = self.g_prime(self.z[self.L - 1]) * self.grad_loss(self.a[self.L - 1], y)\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO: step 3-6. loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        # =================================================================\n",
    "        # Student code here\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.dot(self.delta[ll + 1], self.a[ll].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].T, self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "        \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None,\n",
    "              eta=0.25, num_epochs=10, isPrint=True, isVis=False):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs (step 1.)\n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples (step 2.) \n",
    "            for ind in shuffled_inds: \n",
    "                # =================================================================\n",
    "                # TODO\n",
    "                # =================================================================\n",
    "                # Student code here\n",
    "                \n",
    "                \n",
    "                # TODO: step 3. back prop to get derivatives \n",
    "                # your code here\n",
    "                x, y = X_train[ind], y_train[ind]\n",
    "                \n",
    "                \n",
    "                # TODO: step 4. update all weights and biases for all layers\n",
    "                # your code here\n",
    "                for l in range(self.L-1):\n",
    "                    self.W[l] -= eta * self.dW[l]\n",
    "                    self.b[l] -= eta * self.db[l]\n",
    "                \n",
    "                \n",
    "            # print mean loss every 10 epochs if requested \n",
    "            if isPrint and (ep % 10) == 0:\n",
    "                print(\"epoch {:3d}/{:3d}: \".format(ep, num_epochs), end=\"\")\n",
    "                print(\"  train loss: {:8.3f}\".format(self.compute_loss(X_train, y_train)), end=\"\")\n",
    "                if X_valid is not None:\n",
    "                    print(\"  validation loss: {:8.3f}\".format(self.compute_loss(X_valid, y_valid)))\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    \n",
    "            if isVis and (ep % 20) == 0:\n",
    "                self.pretty_pictures(X_train, y_train, decision_boundary=True, epoch=ep)\n",
    "\n",
    "\n",
    "    # other methods in the starter code.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accompanying Written Exercises\n",
    "\n",
    "**Part A**\n",
    "\n",
    "State whether a single-layer perceptron can learn the concepts. Briefly justify your response by providing weights and biases as applicable.\n",
    "\n",
    "**Note:** Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize indicator activation functions.\n",
    "\n",
    "- NOT $x_1$\n",
    "\n",
    "- $x_1$ NOR $x_2$\n",
    "\n",
    "- $x_1$ XNOR $x_2$ (output 1 when $x_1$ = $x_2$ and 0 otherwise.)\n",
    "\n",
    "\n",
    "**Part B**\n",
    "\n",
    "Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with indicator activation functions that can learn $x_1$ XNOR $x_2$. Describe your architecture and state your weight matrices and bias vectors.\n",
    "\n",
    "Use the text box below for your solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
